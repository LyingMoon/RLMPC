# Predictive RL for MPC: Adapting to Model Parameter Variations
## Introduction
Example code and data for paper **Predictive RL for MPC: Adapting to Model Parameter Variations**


## 1. Set up your MPC
**QubeMPCSetup.m**, this code will setup the State Space Representation of the Qube Servo2 Model

**MPCsuccessful1.slx** is an example simulink model that can run in real world using Qube-Servo 2 Quanser interface, and it can make sure the MPC is working in physical world

**mpcsuccess2.mat** is a successfully trained MPC model


## 2. SMPC and NNMPC Training Set Generation
**001MPC4.mat** A working MPC generated by mpcDesigner in MATLAB

**001SMPC2.pth** A working NNMPC trained in Python

**AdBd001Qube.mat** The ABCD matrix of Qube-Servo2 State Space representation

**GetTrainSet001.m** The code is used to do prediction shirinkage and generate the training set for NNMPC

**MPCvsSMPC001.m** An example code comparing original MPC and SMPC (MPC with prediction shrinkage technique)

**NNnetwork.py** The structure of neuron network used

**PredictionShrinkage.m** An example code illustrating how prediction shirinkage technique work

**TrainingNNMPC.py** The code is used to train a NNMPC using collected dataset

**costfun.m** A cost function used for comparsion in **MPCvsSMPC001.m**


## 3. RLMPC Training in Simulation
**RLQubeModel.py** The simulation model


### 3.1 RL + MPC
**TargetActor1.pth** and **TargetCritic1.pth** are the Actor Network and the Critic Network trained in Simulation for RL + MPC appoarch

**TrainCritic.py** The code is used to train a reliable critic network in simulation

**TrainedActorDDPG.py** The is the DDPG framework, and the actor network is initialized with **001SMPC2.pth**


### 3.2 Warm Start RL
**TargetActorini4.pth** and **TargetCriticini4.pth** are the Actor Network and the Critic Network trained in Simulation for Warm Start RL appoarch

**TrainCriticini.py** The code is used to train a reliable critic network in simulation

**TrainedActorDDPGini.py** The is the DDPG framework, and the actor network is initialized with **001SMPC2.pth**


## 4. RLMPC Training in Real World
**Servo_2_class.py** The Python interface for Qube-Servo 2


### 4.1 RL + MPC
**TargetActor1.pth** and **TargetCritic1.pth** are the Actor Network and the Critic Network trained in Simulation for RL + MPC appoarch

**TrainCritic.py** The code is used to train a reliable critic network in Real World

**TrainedActorDDPG.py** The is the DDPG framework, and the actor network is initialized with **TargetActor1.pth** and **TargetCritic1.pth**, notice that you can replace it with your pre-trained networks

**TargetActorReal1.pth**, **TargetCriticReal1.pth**, **TrainActorReal1.pth**, **TrainCriticReal1.pth** are the networks trained in Real World setting

### 4.2 Warm Start RL
**TargetActorini4.pth** and **TargetCriticini4.pth** are the Actor Network and the Critic Network trained in Simulation for Warm Start RL appoarch

**TrainCriticini.py** The code is used to train a reliable critic network in Real World

**TrainedActorDDPGini.py** The is the DDPG framework, and the actor network is initialized with **TargetActorini4.pth** and **TargetCriticini4.pth**, notice that you can replace it with your pre-trained networks

**TargetActoriniReal1.pth**, **TargetCriticiniReal1.pth**, **TrainActoriniReal1.pth**, **TrainCriticiniReal1.pth** are the networks trained in Real World setting


## 5. The Result Plot
**NLC_IP_SERVO2.py**, a code collecting data from real world RL, when Qube Servo 2 is operating

**PlotRealRL.m**, a code plotting the data collected

**TestRLMPC.py**, a code plotting the performance of RLMPC in simulation, making sure the trained network is working

**TimeTest/MPCRuntime.m**, an example code that evaluate the time required for MPC to go through 400000 (human defined) input set

**TimeTest/TimeTest.py**, and example code that evaluate the time required for NNMPC, Warm Start RL, RL + MPC to go through 400000 (based on the dataset generated using **MPCRuntime.m**) input set


